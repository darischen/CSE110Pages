{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neural Network Beam Profiling Tutorial.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darischen/CSE110Pages/blob/main/Neural_Network_Beam_Profiling_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Training Detectron2 to profile laser beams\n",
        "\n",
        "<img src=\"https://github.com/Dipolar-Quantum-Gases/nn-beam-profiling/blob/master/imgs/thumbnail.jpg?raw=true\" alt=\"drawing\" width=\"150\"/> <img src=\"https://github.com/Dipolar-Quantum-Gases/nn-beam-profiling/blob/master/imgs/thumbnailexp.png?raw=true\" alt=\"drawing\" width=\"150\"/>\n",
        "\n",
        "This notebook demonstrates how to train a deep neural network to detect and profile multiple laser beams in a single image. This is based off the paper \"Measuring Laser Beams with a Neural Network\" which can be found in both [ArXiv](https://arxiv.org/abs/2202.07801) and [published versions](https://doi.org/10.1364/AO.443531)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Package Installation"
      ],
      "metadata": {
        "id": "HDhkwByyODRz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "### Install Detectron2\n",
        "\n",
        "Detectron2 installation needs to match the PyTorch and CUDA versions on Colab's virtual machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXBJO-SPXlCf",
        "outputId": "c158714d-3008-49bb-819e-92656e631ec0"
      },
      "source": [
        "!pip install detectron2 # -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'detectron2#': Expected end or semicolon (after name and no valid version specifier)\n",
            "    detectron2#\n",
            "              ^\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clone Github repository\n",
        "Let's clone the Github repo so we can use the code in the Colab notebook."
      ],
      "metadata": {
        "id": "kthUR3gTAw_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Dipolar-Quantum-Gases/nn-beam-profiling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI-ZYA0QAxHY",
        "outputId": "e77f4190-a09f-47cb-ff13-b53c510b0932"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nn-beam-profiling'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 40 (delta 11), reused 30 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 1.25 MiB | 6.90 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laser beam datasets\n",
        "\n",
        "Their are two datasets used in the paper. The first is a simulated dataset comprised of 5000 images and the second is an experimental dataset with 1050 images created using a spatial light modulator. These are both located in an [University of Oxford Research Archive](https://ora.ox.ac.uk/objects/uuid:e7f9ee4c-5b07-469f-979c-73b8a28d7ec2) from which they can be downloaded as .zip files.\n",
        "\n",
        "For more information on the dataset, we have a [Colab notebook](https://colab.research.google.com/github/Dipolar-Quantum-Gases/nn-beam-profiling/blob/master/Explore_the_Dataset.ipynb) which shows to inspect and visualize the two datasets"
      ],
      "metadata": {
        "id": "popGfx3UAVKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download the dataset"
      ],
      "metadata": {
        "id": "tlHzOPNN_eT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we download and unzip either the experimental or simulation dataset. Note the simulation dataset is 10x the size of the experimental dataset and can take a significant amount of time to download."
      ],
      "metadata": {
        "id": "RTotkx9d_myn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append('/content/nn-beam-profiling/nn-beam-profiling')\n",
        "from dataset_extraction import dataset_extractor\n",
        "\n",
        "#the .zip urls of the two datasets\n",
        "dataset_urls = {'experimental_data': \"https://ora.ox.ac.uk/objects/uuid:e7f9ee4c-5b07-469f-979c-73b8a28d7ec2/download_file?safe_filename=experimental_data.zip&type_of_work=Dataset\",\n",
        "                'simulation_data': \"https://ora.ox.ac.uk/objects/uuid:e7f9ee4c-5b07-469f-979c-73b8a28d7ec2/download_file?safe_filename=simulation_data.zip&type_of_work=Dataset\"}\n",
        "\n",
        "zip_dir = Path('/content/data/datasets/compressed_data')\n",
        "unzip_dir = Path('/content/data/datasets')\n",
        "dataset = 'experimental_data'\n",
        "# dataset = 'simulation_data'\n",
        "\n",
        "url = dataset_urls[dataset]\n",
        "dataset_extractor(dataset, url, zip_dir, unzip_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "2kSYtl-njZJX",
        "outputId": "4ef9c3d0-cb5c-4c2e-ce76-a47d8d9d60c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dataset_extraction'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-83f78f4e932d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/nn-beam-profiling/nn-beam-profiling'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#the .zip urls of the two datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset_extraction'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get the paths to the different parts of the dataset including the annotations (text_path), the rgb images (img_path) and the top level directory (dataset_path)."
      ],
      "metadata": {
        "id": "dZ7sKMb1Pncl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paths(dataset, local_data_path):\n",
        "  dataset_path = local_data_path /  dataset\n",
        "  text_path = dataset_path / 'text'\n",
        "  img_path = dataset_path / 'imgs'\n",
        "  return dataset_path, text_path, img_path\n",
        "\n",
        "dataset_path, text_path, img_path = get_paths(dataset, unzip_dir)"
      ],
      "metadata": {
        "id": "Nb4Z9Pd5PnkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Detectron2 dataset\n",
        "\n",
        "Now we define a dataset class for our custom laser beam datasets. The dataframe file holds all the annotation information including boxes, rotated boxes, masks and keypoints. However here we focus on the rotated boxes."
      ],
      "metadata": {
        "id": "bDzCLO8bBSed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "import pandas as pd\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "class BeamDataset():\n",
        "\n",
        "    def __init__(self, dataset_path, text_path, scalar=1):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.text_path = text_path\n",
        "        self.box_name = 'abox_' + str(scalar)\n",
        "\n",
        "\n",
        "    def training_dataset(self):\n",
        "        return self.get_dataset_list()\n",
        "\n",
        "\n",
        "    def validation_dataset(self):\n",
        "        return self.get_dataset_list(False)\n",
        "\n",
        "\n",
        "    def read_DF(self, train):\n",
        "        if train:\n",
        "          self.data = pd.read_json(str(self.text_path) + '/data_train.json')\n",
        "        else:\n",
        "          self.data = pd.read_json(str(self.text_path) + '/data_val.json')\n",
        "\n",
        "\n",
        "    def get_dataset_list(self, train=True):\n",
        "        self.read_DF(train)\n",
        "        # dlist = []\n",
        "        dlist = [self.getitem(i, train) for i in range(len(self.data))]\n",
        "        # length = len(self.data)\n",
        "        # for i in range(0, length):\n",
        "        #   dlist.append(self.getitem(i, train))\n",
        "        return dlist\n",
        "\n",
        "\n",
        "    def dataset_length(self, train=True):\n",
        "        self.read_DF(train)\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getitem(self, idx, train=True):\n",
        "        # load images and ROI boxes\n",
        "        idata = self.data.loc[idx]\n",
        "        aboxes = idata[self.box_name]\n",
        "        labels = idata['label']\n",
        "        annotations = []\n",
        "        for abox, label in zip(aboxes, labels): # get bounding box coordinates and fits for each mask\n",
        "            annotation_dict = {}\n",
        "            annotation_dict['bbox'] = [float(val) for val in abox]\n",
        "            annotation_dict['bbox_mode'] = BoxMode.XYWHA_ABS\n",
        "            annotation_dict['category_id'] = label - 1\n",
        "            annotation_dict['iscrowd'] = 0\n",
        "            annotations.append(annotation_dict)\n",
        "\n",
        "        image_dict = {}\n",
        "        image_dict['annotations'] = annotations\n",
        "        file_name = str(self.dataset_path) + idata['rgb_paths'].replace('\\\\', '/')\n",
        "        image_dict['file_name'] = file_name\n",
        "        image_dict['image_id'] = int(idata['run'])\n",
        "        image_dict['height'] = idata['height']\n",
        "        image_dict['width'] = idata['width']\n",
        "\n",
        "        return image_dict"
      ],
      "metadata": {
        "id": "Wj30Q-jtqn2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "### Registering the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV42gPp3o-b-"
      },
      "source": [
        "Detectron2 requires us to register our datasets and their metadata before we can use them. This is done for both the training and validation datsets in the code below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "def register_dataset_metadata(dataset_name, dataset_function, class_names):\n",
        "  if dataset_name in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(dataset_name)\n",
        "    MetadataCatalog.remove(dataset_name)\n",
        "  DatasetCatalog.register(dataset_name, dataset_function)\n",
        "  MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "\n",
        "\n",
        "train_dataset = 'train_dataset'\n",
        "val_dataset = 'val_dataset'\n",
        "dataset_scalar = 1.5\n",
        "class_names = [\"Gaussian\"]\n",
        "acd = BeamDataset(dataset_path, text_path, scalar=dataset_scalar)\n",
        "register_dataset_metadata(train_dataset, acd.training_dataset, class_names)\n",
        "register_dataset_metadata(val_dataset, acd.validation_dataset, class_names)"
      ],
      "metadata": {
        "id": "c_CXea2Lq38h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJl213SnpGid"
      },
      "source": [
        "###Visualize the dataset\n",
        "\n",
        "Here we randomly visualize some of the validation dataset images along with their annotations for rotated regions-of-interest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "source": [
        "import random\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def get_dataset_info(dataset_name):\n",
        "  dataset_metadata = MetadataCatalog.get(dataset_name)\n",
        "  dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "  return dataset_metadata, dataset_dicts\n",
        "\n",
        "\n",
        "def visualize_dataset(dataset_metadata, dataset_dicts, num_samples=5, scale = 2):\n",
        "  dlength = len(dataset_dicts)\n",
        "  if num_samples > dlength:\n",
        "    num_samples = dlength\n",
        "  for d in random.sample(dataset_dicts, num_samples):\n",
        "      img = cv2.imread(d[\"file_name\"])\n",
        "      visualizer = Visualizer(img[:, :, ::-1], metadata=dataset_metadata, scale=scale)\n",
        "      vis = visualizer.draw_dataset_dict(d)\n",
        "      cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "\n",
        "\n",
        "training_dataset_metadata, training_dataset_dicts = get_dataset_info(train_dataset)\n",
        "visualize_dataset(training_dataset_metadata, training_dataset_dicts, num_samples=2)\n",
        "\n",
        "val_dataset_metadata, val_dataset_dicts = get_dataset_info(val_dataset)\n",
        "visualize_dataset(val_dataset_metadata, val_dataset_dicts, num_samples=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural network model\n",
        "\n",
        "Rather than building the model explicitly in PyTorch, we modify a config file (CFG) to define the neural network we want built. Once we enter the training loop, Detectron2 will build the neural network model according to the CFG specifications."
      ],
      "metadata": {
        "id": "TfacZEQk5nh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# get_base_model_and_weights\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "# setup_rpn_head\n",
        "cfg.MODEL.PROPOSAL_GENERATOR.NAME = \"RRPN\"\n",
        "cfg.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"\n",
        "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (1, 1, 1, 1, 1)\n",
        "\n",
        "# setup_anchor_generator\n",
        "cfg.MODEL.ANCHOR_GENERATOR.NAME = \"RotatedAnchorGenerator\"\n",
        "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-60,-30,0,30,60,90]]\n",
        "\n",
        "# Setup RROI heads\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .5\n",
        "cfg.MODEL.ROI_HEADS.NAME = \"RROIHeads\"\n",
        "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
        "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (1, 1, 1, 1, 1)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
        "\n",
        "cfg.MODEL.MASK_ON = False\n",
        "cfg.DATALOADER.NUM_WORKERS = 2"
      ],
      "metadata": {
        "id": "Vxl8UIylMJI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "AIWJCDdW2oTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom Detectron2 trainer\n",
        "\n",
        "The default Detectron2 trainer doesn't work for rotated regions-of-interest so we write a custom trainer. This trainer needs a custom data mapper to be implemented in both the training and test loader.\n",
        "\n",
        "Additionally, the evaluation between training epochs must be done with the Rotated COCO evaluator."
      ],
      "metadata": {
        "id": "-Z1-taH-2JW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.evaluation import DatasetEvaluators, RotatedCOCOEvaluator\n",
        "from rotation_mapper import MyDatasetMapper\n",
        "\n",
        "\n",
        "class MyTrainer(DefaultTrainer):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__(cfg)\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def build_evaluator(self, cfg, dataset_name, output_folder=None):\n",
        "      if output_folder is None:\n",
        "          output_folder = cfg.OUTPUT_DIR + '/coco_eval/' + dataset_name\n",
        "          os.makedirs(output_folder, exist_ok=True)\n",
        "      evaluators = [RotatedCOCOEvaluator(dataset_name, cfg, True, output_folder)]\n",
        "      return DatasetEvaluators(evaluators)\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def build_train_loader(cls, cfg, hf=False, height=800, width=800):\n",
        "    augmentations = T.AugmentationList([T.Resize((height, width))])\n",
        "    return build_detection_train_loader(cfg, mapper=MyDatasetMapper(cfg, True,\n",
        "                                                                    augmentations))\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def build_test_loader(cls, cfg, dataset, height=800, width=800):\n",
        "    augmentations = T.AugmentationList([T.Resize((height, width))])\n",
        "    return build_detection_test_loader(cfg, dataset, mapper=MyDatasetMapper(cfg,\n",
        "                                                                            False,\n",
        "                                                                            augmentations))"
      ],
      "metadata": {
        "id": "K3CgBn0F1C6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set training parameters\n",
        "\n",
        "Here we set the hyperparameters for training the neural network including batch size, learning rate, learning rate scheduler, momentum and training epochs.\n",
        "\n",
        "Detectron2 uses iterations (number of batches passed through the NN) rather than training epochs so we need to convert between the two."
      ],
      "metadata": {
        "id": "bQnr_zmJ5sqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "\n",
        "def get_learning_rate_schedule(lrs_iters, train_iters):\n",
        "    max_steps = int(ceil(train_iters / lrs_iters))\n",
        "    schedule = [lrs_iters * (i + 1) - 1 for i in range(0, max_steps)]\n",
        "    return schedule\n",
        "\n",
        "\n",
        "def epochs_to_iterations(epochs, num_imgs, batch_size):\n",
        "    return int(epochs * (num_imgs / batch_size))\n",
        "\n",
        "# define the parameters for training\n",
        "train_epochs = 2\n",
        "eval_epochs = 1\n",
        "batch_size = 4\n",
        "lrs_step = 10\n",
        "num_imgs = len(DatasetCatalog.get(train_dataset))\n",
        "\n",
        "# convert epochs to iterations\n",
        "train_iters = epochs_to_iterations(train_epochs, num_imgs, batch_size)\n",
        "eval_iters = epochs_to_iterations(eval_epochs, num_imgs, batch_size)\n",
        "lrs_iters = epochs_to_iterations(lrs_step, num_imgs, batch_size)\n",
        "lrs_schedule = get_learning_rate_schedule(lrs_iters, train_iters)\n",
        "\n",
        "# set training hyperparameters in the config file\n",
        "cfg.SOLVER.IMS_PER_BATCH = batch_size\n",
        "cfg.SOLVER.BASE_LR = 0.01\n",
        "cfg.SOLVER.MOMENTUM = 0.9\n",
        "cfg.SOLVER.GAMMA = .01\n",
        "cfg.SOLVER.STEPS = lrs_schedule\n",
        "cfg.SOLVER.WARMUP_ITERS = 0\n",
        "cfg.SOLVER.MAX_ITER = train_iters\n",
        "cfg.TEST.EVAL_PERIOD = eval_iters\n",
        "\n",
        "# set training and validation datasets\n",
        "cfg.DATASETS.TRAIN = (train_dataset,)\n",
        "cfg.DATASETS.TEST = (val_dataset,)"
      ],
      "metadata": {
        "id": "vU9SSVF3xipZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the neural network\n",
        "\n",
        "Now we create folder for all the training/evaluation results and start the training loop. The custom trainer we defined earlier will construct the neural network according to the CFG file and then start the training loop with the hyperparameters also defined in the CFG file."
      ],
      "metadata": {
        "id": "zgxgK0z6DogN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "def make_output_dir(dataset, base_dir):\n",
        "  output_dir = base_dir + '/' + dataset + '/' + time.strftime('%H''%M''_''%d''%m''%Y')\n",
        "  cfg.OUTPUT_DIR = output_dir\n",
        "  if os.path.isdir(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "make_output_dir(dataset, '/content/training_output')\n",
        "trainer = MyTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HbB5Kpkm2kAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate"
      ],
      "metadata": {
        "id": "kzIYyYs7GvVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get the training/evaluation results\n",
        "\n",
        "Let's first get the metrics file that Detectron saves throughout training. Detectron 2 spits out a training/validation metrics .json file. This is kind of a jumble and here we parse it to get out a training dataframe and an evaluation dataframe."
      ],
      "metadata": {
        "id": "F1GhbwWaKzCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sub_df(metricsDF, split_string):\n",
        "  sub_df = metricsDF.dropna(subset=[split_string]) #split validation metric\n",
        "  sub_df = sub_df.dropna(axis=1, how='any') #drop columns with na values\n",
        "  sub_df = sub_df.sort_values(by=['iteration']) #sort by training iteration\n",
        "  sub_df = sub_df.reset_index(drop=True)\n",
        "  return sub_df\n",
        "\n",
        "\n",
        "def get_metrics(metrics_path):\n",
        "  '''Does all the splitting of the metrics df into its respective components.'''\n",
        "  metricsDF = pd.read_json(metrics_path + '/metrics.json', lines=True)\n",
        "  training_df = get_sub_df(metricsDF, 'total_loss')\n",
        "  evaluation_df = get_sub_df(metricsDF, 'bbox/APs')\n",
        "  return training_df, evaluation_df\n",
        "\n",
        "training_df, evaluation_df = get_metrics(cfg.OUTPUT_DIR)\n",
        "evaluation_df"
      ],
      "metadata": {
        "id": "JeovbmDxE2dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot the evaluation results\n",
        "\n",
        "We plot the mAP after each training epoch and the loss during training."
      ],
      "metadata": {
        "id": "g62M8xdWKee2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df.plot('iteration', 'bbox/AP')\n",
        "training_df.plot('iteration', 'total_loss')"
      ],
      "metadata": {
        "id": "h7GW0ffFGJGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the trained model"
      ],
      "metadata": {
        "id": "9XRIqlxYUgvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference\n",
        "\n",
        "We want to use the trained neural network on images which it was not trained on. Below we create an inference predictor with weights from the trained model.\n",
        "\n",
        "We then randomly select images from the validation dataset and perform inference on them."
      ],
      "metadata": {
        "id": "cYqui2mFUNQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "def pred_abox_list(outputs):\n",
        "  tensor_pred = outputs['instances'].get_fields()['pred_boxes'].tensor\n",
        "  pred_list = tensor_pred.cpu().numpy().tolist()\n",
        "  return pred_list\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST =0.8\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "num_samples = 5\n",
        "for d in random.sample(val_dataset_dicts, num_samples):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=val_dataset_metadata)\n",
        "    outputs = predictor(img)\n",
        "    v = Visualizer(img[:,:,::-1], val_dataset_metadata, scale=2)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "    print('Ground Truth')\n",
        "    [print(pobject['bbox']) for pobject in d['annotations']]\n",
        "    print('Prediction')\n",
        "    [print(box) for box in pred_abox_list(outputs)]"
      ],
      "metadata": {
        "id": "gj9YW1bt7y2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAcL7iJ9UDLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}